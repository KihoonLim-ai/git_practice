# Ablation Study Models

5ê°€ì§€ Physics-Informed DeepONet ë³€í˜• ëª¨ë¸ë“¤ì„ ë°”ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“‹ ëª¨ë¸ ëª©ë¡

| ëª¨ë¸ íŒŒì¼ | ì„¤ëª… | íŠ¹ì§• |
|----------|------|------|
| `model_baseline.py` | ê¸°ë³¸ ëª¨ë¸ (PDE ì—†ìŒ) | í˜„ì¬ ST_TransformerDeepONet |
| `model_soft_pde.py` | Soft PDE ì œì•½ | PDE residualì„ lossì— ì¶”ê°€ |
| `model_annealed_pde.py` | Epoch-based Annealing | PDE weightê°€ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ (0.1â†’1.0) |
| `model_adaptive_pde.py` | Adaptive Weighting (ReLoBRaLo) | ìë™ loss ê°€ì¤‘ì¹˜ ì¡°ì • |
| `model_hard_pde.py` | Hard Constraints | Output transformationìœ¼ë¡œ ì œì•½ ê°•ì œ |

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. Baseline Model (ê¸°ì¤€ì„ )

```python
from model.model_baseline import ST_TransformerDeepONet_Baseline

model = ST_TransformerDeepONet_Baseline(latent_dim=128)

# Training loop
for batch in dataloader:
    pred_wind, pred_conc = model(ctx_map, obs_seq, coords, global_wind)

    # Standard losses only (no PDE)
    loss = loss_mse + loss_pcc + loss_phys
    loss.backward()
```

---

### 2. Soft PDE Model

```python
from model.model_soft_pde import ST_TransformerDeepONet_SoftPDE

model = ST_TransformerDeepONet_SoftPDE(
    latent_dim=128,
    diffusion_coeff=0.1  # Dê°’ (ëŒ€ê¸° ë‚œë¥˜ í™•ì‚°ê³„ìˆ˜)
)

# Training loop
LAMBDA_PDE = 1.0  # PDE loss weight (ê³ ì •)

for batch in dataloader:
    pred_wind, pred_conc, pde_loss = model(
        ctx_map, obs_seq, coords, global_wind,
        compute_pde_loss=True,  # PDE ê³„ì‚° í™œì„±í™”
        source=source_term      # ë°°ì¶œì› í•­ (optional)
    )

    loss_total = (
        loss_mse +
        loss_pcc +
        loss_phys +
        LAMBDA_PDE * pde_loss  # PDE residual ì¶”ê°€
    )
    loss_total.backward()
```

**ì£¼ì˜**: `coords`ëŠ” `requires_grad=True`ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤!
```python
coords = make_batch_coords(...).requires_grad_(True)
```

---

### 3. Annealed PDE Model

```python
from model.model_annealed_pde import ST_TransformerDeepONet_AnnealedPDE

model = ST_TransformerDeepONet_AnnealedPDE(
    latent_dim=128,
    diffusion_coeff=0.1,
    total_epochs=100  # ì „ì²´ epoch ìˆ˜
)

# Training loop
for epoch in range(total_epochs):
    model.set_epoch(epoch)  # í˜„ì¬ epoch ì—…ë°ì´íŠ¸
    lambda_pde = model.get_pde_weight()  # ìë™ weight ê³„ì‚°

    print(f"Epoch {epoch}: PDE weight = {lambda_pde:.3f}")

    for batch in dataloader:
        pred_wind, pred_conc, pde_loss = model(
            ctx_map, obs_seq, coords, global_wind,
            compute_pde_loss=True
        )

        loss_total = (
            loss_mse +
            loss_pcc +
            loss_phys +
            lambda_pde * pde_loss  # Annealed weight
        )
        loss_total.backward()
```

**Annealing Schedule**:
- Epoch 0-30: `Î»_pde = 0.1` (ë°ì´í„° í•™ìŠµ ì§‘ì¤‘)
- Epoch 30-70: `Î»_pde = 0.1 â†’ 1.0` (ì„ í˜• ì¦ê°€)
- Epoch 70-100: `Î»_pde = 1.0` (ë¬¼ë¦¬ ì œì•½ ì™„ì „ ì ìš©)

---

### 4. Adaptive PDE Model (ReLoBRaLo)

```python
from model.model_adaptive_pde import ST_TransformerDeepONet_AdaptivePDE

model = ST_TransformerDeepONet_AdaptivePDE(
    latent_dim=128,
    diffusion_coeff=0.1,
    lookback=10  # Loss history ê¸¸ì´
)

# Training loop
for epoch in range(total_epochs):
    for batch in dataloader:
        pred_wind, pred_conc, pde_loss = model(
            ctx_map, obs_seq, coords, global_wind,
            compute_pde_loss=True
        )

        # ëª¨ë“  loss ê³„ì‚°
        loss_dict = {
            'mse': loss_mse,
            'pcc': loss_pcc,
            'phys': loss_phys,
            'pde': pde_loss
        }

        # ìë™ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¡°ì •
        weights = model.compute_adaptive_weights(loss_dict)

        # ê°€ì¤‘ í•©ì‚°
        loss_total = sum(weights[k] * loss_dict[k] for k in loss_dict)
        loss_total.backward()

        # Logging
        if step % 100 == 0:
            print(f"Adaptive weights: {weights}")
```

**ì¥ì **:
- ìë™ ê°€ì¤‘ì¹˜ ì¡°ì • (hyperparameter íŠœë‹ ë¶ˆí•„ìš”)
- ë¹ ë¥´ê²Œ ë³€í•˜ëŠ” lossì— ë‚®ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
- ëŠë¦¬ê²Œ ë³€í•˜ëŠ” lossì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬

---

### 5. Hard PDE Model (Output Transform)

```python
from model.model_hard_pde import ST_TransformerDeepONet_HardPDE

model = ST_TransformerDeepONet_HardPDE(
    latent_dim=128,
    diffusion_coeff=0.1,
    use_soft_pde=True  # Hybrid: hard + soft constraints
)

# Training loop
for batch in dataloader:
    pred_wind, pred_conc, pde_loss = model(
        ctx_map, obs_seq, coords, global_wind,
        compute_pde_loss=True,         # Soft PDE lossë„ ê³„ì‚° (hybrid)
        apply_hard_constraints=True     # Hard constraints ì ìš©
    )

    loss_total = loss_mse + loss_pcc + loss_phys + pde_loss
    loss_total.backward()
```

**Hard Constraints ê°•ì œ ì‚¬í•­**:
- âœ… ì§€í˜• ë‚´ë¶€ì—ì„œ ë†ë„ = 0 (ìë™)
- âœ… ë†ë„ â‰¥ 0 (í•­ìƒ non-negative)

**Hybrid ëª¨ë“œ** (`use_soft_pde=True`):
- Hard constraints (ë³´ì¥ë¨) + Soft PDE loss (í•™ìŠµ ê°€ì´ë“œ)
- ë” ë¹ ë¥¸ ìˆ˜ë ´ ê¸°ëŒ€

---

## ğŸ“Š Ablation Study ì‹¤í–‰ ì˜ˆì œ

```python
# train_ablation.py
import argparse

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, required=True,
                        choices=['baseline', 'soft_pde', 'annealed_pde',
                                'adaptive_pde', 'hard_pde'])
    args = parser.parse_args()

    # ëª¨ë¸ ì„ íƒ
    if args.model == 'baseline':
        from model.model_baseline import ST_TransformerDeepONet_Baseline
        model = ST_TransformerDeepONet_Baseline()
    elif args.model == 'soft_pde':
        from model.model_soft_pde import ST_TransformerDeepONet_SoftPDE
        model = ST_TransformerDeepONet_SoftPDE(diffusion_coeff=0.1)
    elif args.model == 'annealed_pde':
        from model.model_annealed_pde import ST_TransformerDeepONet_AnnealedPDE
        model = ST_TransformerDeepONet_AnnealedPDE(total_epochs=100)
    elif args.model == 'adaptive_pde':
        from model.model_adaptive_pde import ST_TransformerDeepONet_AdaptivePDE
        model = ST_TransformerDeepONet_AdaptivePDE(lookback=10)
    elif args.model == 'hard_pde':
        from model.model_hard_pde import ST_TransformerDeepONet_HardPDE
        model = ST_TransformerDeepONet_HardPDE(use_soft_pde=True)

    # Train...
    print(f"Training {args.model} model...")

if __name__ == '__main__':
    main()
```

**ì‹¤í–‰**:
```bash
# 5ê°€ì§€ ëª¨ë¸ ëª¨ë‘ í•™ìŠµ
python train_ablation.py --model baseline
python train_ablation.py --model soft_pde
python train_ablation.py --model annealed_pde
python train_ablation.py --model adaptive_pde
python train_ablation.py --model hard_pde
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ì§€í‘œ

í•™ìŠµ í›„ ë‹¤ìŒ ì§€í‘œë¡œ ë¹„êµ:

| Metric | ì„¤ëª… | ëª©í‘œ |
|--------|------|------|
| **Data MSE** | ì˜ˆì¸¡ vs ì‹¤ì œ ë†ë„ MSE | ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ |
| **PCC** | Pattern Correlation Coefficient | ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ |
| **PDE Residual** | PDE ìœ„ë°˜ ì •ë„ | < 1e-3 |
| **Continuity Loss** | âˆ‡Â·u divergence | < 1e-3 |
| **Convergence Speed** | ìˆ˜ë ´ê¹Œì§€ epoch ìˆ˜ | ì ì„ìˆ˜ë¡ ì¢‹ìŒ |

---

## ğŸ”¬ ì˜ˆìƒ ê²°ê³¼

| ëª¨ë¸ | Data MSE | PDE Residual | ìˆ˜ë ´ ì†ë„ | ì¥ì  |
|------|----------|--------------|----------|------|
| Baseline | ê¸°ì¤€ | N/A | ë³´í†µ | êµ¬í˜„ ë‹¨ìˆœ |
| Soft PDE | ê¸°ì¤€ | ~1e-2 | ëŠë¦¼ | ë¬¼ë¦¬ ì œì•½ ì¶”ê°€ |
| Annealed PDE | **ìµœì €** | ~1e-3 | **ë¹ ë¦„** | ì•ˆì •ì  ìˆ˜ë ´ |
| Adaptive PDE | ë‚®ìŒ | ~1e-3 | ë¹ ë¦„ | ìë™ íŠœë‹ |
| Hard PDE | ë³´í†µ | **0 (ë³´ì¥)** | ë³´í†µ | ì œì•½ ë³´ì¥ |

**ì¶”ì²œ ìˆœì„œ**:
1. **Annealed PDE**: ê°€ì¥ ê· í˜• ì¡íŒ ì„±ëŠ¥ ì˜ˆìƒ
2. **Adaptive PDE**: Hyperparameter íŠœë‹ ë¶ˆí•„ìš”
3. **Hard PDE**: ì œì•½ ë§Œì¡±ì´ ì¤‘ìš”í•œ ê²½ìš°

---

## âš™ï¸ Hyperparameter ê¶Œì¥ê°’

### Diffusion Coefficient (D)
```python
diffusion_coeff = 0.1  # ëŒ€ê¸° ë‚œë¥˜ (0.01 ~ 0.5 ë²”ìœ„)
```

### Annealing Schedule
```python
total_epochs = 100
# Phase 1: 0-30 epochs (30%)
# Phase 2: 30-70 epochs (40%)
# Phase 3: 70-100 epochs (30%)
```

### ReLoBRaLo Lookback
```python
lookback = 10  # ìµœê·¼ 10 stepì˜ loss ë³€í™” ì¶”ì 
```

---

## ğŸ› Troubleshooting

### 1. PDE Lossê°€ NaN
**ì›ì¸**: Gradient computation ì‹¤íŒ¨

**í•´ê²°**:
```python
# coordsì— requires_grad ì„¤ì •
coords = make_batch_coords(...).requires_grad_(True)

# Mixed precision ì‚¬ìš© ì‹œ autocast ë¹„í™œì„±í™”
with torch.cuda.amp.autocast(enabled=False):
    pde_loss = model.pde_residual(...)
```

### 2. Memory ë¶€ì¡±
**ì›ì¸**: Second derivatives ê³„ì‚°ì´ ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©

**í•´ê²°**:
```python
# Batch size ì¤„ì´ê¸°
batch_size = 4  # 8 â†’ 4

# ë˜ëŠ” coords ìƒ˜í”Œë§ ìˆ˜ ì¤„ì´ê¸°
coords = make_batch_coords(B, nz=10, ny=22, nx=22)  # 21x45x45 â†’ 10x22x22
```

### 3. Adaptive weightsê°€ ë¶ˆì•ˆì •
**ì›ì¸**: Lookbackì´ ë„ˆë¬´ ì§§ìŒ

**í•´ê²°**:
```python
# Lookback ëŠ˜ë¦¬ê¸°
model = ST_TransformerDeepONet_AdaptivePDE(lookback=20)  # 10 â†’ 20
```

---

## ğŸ“š ì°¸ê³  ë…¼ë¬¸

1. **Physics-Informed DeepONets** (arXiv:2103.10974)
2. **ReLoBRaLo** (arXiv:2110.09813)
3. **Hard Constraints in PINNs** (arXiv:2306.12749)

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

í•™ìŠµ ì „ í™•ì¸:
- [ ] `coords.requires_grad = True` ì„¤ì •
- [ ] Diffusion coefficient ê°’ í™•ì¸ (0.01 ~ 0.5)
- [ ] Total epochs ì„¤ì • (Annealed ëª¨ë¸)
- [ ] Lookback ì„¤ì • (Adaptive ëª¨ë¸)
- [ ] GPU ë©”ëª¨ë¦¬ ì¶©ë¶„í•œì§€ í™•ì¸
- [ ] WandB ë˜ëŠ” TensorBoard ë¡œê¹… ì„¤ì •

í•™ìŠµ ì¤‘ ëª¨ë‹ˆí„°ë§:
- [ ] PDE loss ê°ì†Œ ì¶”ì„¸
- [ ] Data MSE vs PDE loss ê· í˜•
- [ ] Adaptive weights ë³€í™” (Adaptive ëª¨ë¸)
- [ ] Annealing schedule ì§„í–‰ (Annealed ëª¨ë¸)

---

**ì¤€ë¹„ ì™„ë£Œ!** 5ê°€ì§€ ëª¨ë¸ì„ ë°”ë¡œ í•™ìŠµì‹œì¼œ ablation studyë¥¼ ì§„í–‰í•˜ì„¸ìš”! ğŸš€
