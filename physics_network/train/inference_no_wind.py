"""
Inference script for No-Wind Model
- Loads trained checkpoint
- Runs prediction on test set
- Saves results for visualization
"""
import os
import sys

# OpenMP ì¤‘ë³µ ë¡œë“œ ë¬¸ì œ í•´ê²°
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import numpy as np
from tqdm import tqdm

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from dataset.dataset_no_wind import get_dataloaders_no_wind
from dataset.physics_utils import make_batch_coords
from dataset.config_param import ConfigParam as Config
from model.model_no_wind import SimplifiedDeepONet


class InferenceConfig:
    """ì¶”ë¡  ì„¤ì •"""
    CHECKPOINT_PATH = "checkpoints_no_wind/best_no_wind.pth"
    OUTPUT_DIR = "inference_results_no_wind"
    BATCH_SIZE = 4  # ì¶”ë¡  ì‹œì—ëŠ” ì‘ì€ ë°°ì¹˜ë¡œ
    NUM_SAMPLES = 20  # ì €ì¥í•  ìƒ˜í”Œ ê°œìˆ˜ (ì‹œê°í™”ìš©)


def load_model(checkpoint_path, device):
    """
    ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ

    Args:
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        device: torch device

    Returns:
        model: ë¡œë“œëœ ëª¨ë¸
        checkpoint: ì²´í¬í¬ì¸íŠ¸ ë”•ì…”ë„ˆë¦¬
    """
    print(f"ğŸ“‚ Loading checkpoint from: {checkpoint_path}")

    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location=device)

    # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    model_config = checkpoint['config']

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = SimplifiedDeepONet(
        latent_dim=model_config['latent_dim'],
        fourier_scale=model_config['fourier_scale'],
        dropout=model_config['dropout']
    ).to(device)

    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    print(f"âœ… Model loaded from epoch {checkpoint['epoch']}")
    print(f"   Best Val Loss: {checkpoint['best_val_loss']:.6f}")

    return model, checkpoint


def run_inference(model, test_loader, device, num_samples=20):
    """
    í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¶”ë¡  ì‹¤í–‰

    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
        device: torch device
        num_samples: ì €ì¥í•  ìƒ˜í”Œ ê°œìˆ˜

    Returns:
        results: ì¶”ë¡  ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\nğŸ”® Running inference on test set...")

    results = {
        'predictions': [],
        'targets': [],
        'inputs': [],
        'metrics': {
            'mse': [],
            'mae': [],
            'pcc': []
        }
    }

    model.eval()
    sample_count = 0

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(test_loader, desc="Inference")):
            inp_vol, target_vol = [b.to(device) for b in batch]

            # Generate coordinates
            B, C, D, H, W = inp_vol.shape
            coords = make_batch_coords(B, D, H, W, device=device)

            # Forward pass
            pred_conc = model(inp_vol, coords)  # (B, N, 1)

            # Reshape to original volume
            pred_vol = pred_conc.reshape(B, D, H, W)  # (B, 21, 45, 45)

            # Compute metrics per sample
            for i in range(B):
                if sample_count >= num_samples:
                    break

                pred = pred_vol[i].cpu().numpy()  # (21, 45, 45)
                target = target_vol[i, 0].cpu().numpy()  # (21, 45, 45)
                inp = inp_vol[i].cpu().numpy()  # (2, 21, 45, 45)

                # Calculate metrics
                mse = np.mean((pred - target) ** 2)
                mae = np.mean(np.abs(pred - target))

                # Pearson correlation
                pred_flat = pred.flatten()
                target_flat = target.flatten()
                pcc = np.corrcoef(pred_flat, target_flat)[0, 1]

                # Store results
                results['predictions'].append(pred)
                results['targets'].append(target)
                results['inputs'].append(inp)
                results['metrics']['mse'].append(mse)
                results['metrics']['mae'].append(mae)
                results['metrics']['pcc'].append(pcc)

                sample_count += 1

            if sample_count >= num_samples:
                break

    # Convert to numpy arrays
    results['predictions'] = np.array(results['predictions'])
    results['targets'] = np.array(results['targets'])
    results['inputs'] = np.array(results['inputs'])

    # Compute average metrics
    for key in results['metrics']:
        results['metrics'][key] = np.array(results['metrics'][key])

    print(f"\nğŸ“Š Inference Results (on {sample_count} samples):")
    print(f"   Average MSE: {results['metrics']['mse'].mean():.6f}")
    print(f"   Average MAE: {results['metrics']['mae'].mean():.6f}")
    print(f"   Average PCC: {results['metrics']['pcc'].mean():.4f}")

    return results


def save_results(results, output_dir):
    """
    ì¶”ë¡  ê²°ê³¼ ì €ì¥

    Args:
        results: ì¶”ë¡  ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
    """
    os.makedirs(output_dir, exist_ok=True)

    print(f"\nğŸ’¾ Saving results to: {output_dir}")

    # Save predictions and targets
    np.savez_compressed(
        os.path.join(output_dir, "predictions.npz"),
        predictions=results['predictions'],
        targets=results['targets'],
        inputs=results['inputs'],
        mse=results['metrics']['mse'],
        mae=results['metrics']['mae'],
        pcc=results['metrics']['pcc']
    )

    # Save summary statistics
    summary = {
        'num_samples': len(results['predictions']),
        'mean_mse': float(results['metrics']['mse'].mean()),
        'std_mse': float(results['metrics']['mse'].std()),
        'mean_mae': float(results['metrics']['mae'].mean()),
        'std_mae': float(results['metrics']['mae'].std()),
        'mean_pcc': float(results['metrics']['pcc'].mean()),
        'std_pcc': float(results['metrics']['pcc'].std()),
    }

    import json
    with open(os.path.join(output_dir, "summary.json"), 'w') as f:
        json.dump(summary, f, indent=2)

    print(f"âœ… Saved:")
    print(f"   - predictions.npz (predictions, targets, inputs, metrics)")
    print(f"   - summary.json (statistics)")


def main():
    """ë©”ì¸ ì¶”ë¡  ì‹¤í–‰"""
    cfg = InferenceConfig()

    print("=" * 70)
    print("ğŸ”® No-Wind Model Inference")
    print("=" * 70)

    # Device ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ–¥ï¸ Using device: {device}")

    # ëª¨ë¸ ë¡œë“œ
    model, checkpoint = load_model(cfg.CHECKPOINT_PATH, device)

    # ë°ì´í„° ë¡œë” ìƒì„± (í…ŒìŠ¤íŠ¸ì…‹ë§Œ)
    print("\nğŸ“¦ Loading test data...")
    _, _, test_loader = get_dataloaders_no_wind(
        batch_size=cfg.BATCH_SIZE,
        crop_size=45,  # Full resolution
        num_workers=0
    )

    # ì¶”ë¡  ì‹¤í–‰
    results = run_inference(
        model=model,
        test_loader=test_loader,
        device=device,
        num_samples=cfg.NUM_SAMPLES
    )

    # ê²°ê³¼ ì €ì¥
    save_results(results, cfg.OUTPUT_DIR)

    print("\n" + "=" * 70)
    print("ğŸ‰ Inference completed!")
    print("=" * 70)
    print(f"\nNext steps:")
    print(f"  1. Run visualization: python visualize_no_wind.py")
    print(f"  2. Check results in: {cfg.OUTPUT_DIR}/")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Inference interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Error occurred: {e}")
        import traceback
        traceback.print_exc()
