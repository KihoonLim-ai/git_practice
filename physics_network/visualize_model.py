"""
Model Architecture Visualization Script
ëª¨ë¸ êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¡œ ì‹œê°í™”
"""
import os
import sys

# Graphviz PATH ì¶”ê°€ (Windows)
graphviz_bin = r"C:\Program Files\Graphviz\bin"
if os.path.exists(graphviz_bin):
    os.environ["PATH"] += os.pathsep + graphviz_bin

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

import torch#; import torchinfo; import torchviz
from model.model_seq2seq_v2 import ConcentrationSeq2Seq_v2

# ============================================
# Method 1: torchinfo (ê°€ì¥ ì¶”ì²œ!)
# ============================================
def visualize_with_torchinfo():
    """torchinfoë¥¼ ì‚¬ìš©í•œ ìƒì„¸í•œ êµ¬ì¡° ì¶œë ¥"""
    try:
        from torchinfo import summary

        print("=" * 80)
        print("ğŸ“Š Model Architecture Summary (torchinfo)")
        print("=" * 80)

        # ëª¨ë¸ ìƒì„±
        model = ConcentrationSeq2Seq_v2(
            hidden_channels=32,
            num_lstm_layers=2,
            output_shape=(21, 45, 45)
        )

        # ì…ë ¥ shape ì •ì˜
        batch_size = 8
        seq_len = 30

        # Summary ì¶œë ¥
        summary(
            model,
            input_size=[
                (batch_size, seq_len, 21, 45, 45),  # past_conc
                (batch_size, 2, 21, 45, 45)         # static_maps
            ],
            col_names=["input_size", "output_size", "num_params", "kernel_size"],
            depth=5,
            device="cpu"
        )

        print("\nâœ… torchinfo summary completed!")
        return True

    except ImportError:
        print("âš ï¸ torchinfo not installed. Install with: pip install torchinfo")
        return False


# ============================================
# Method 2: torchviz (ê·¸ë˜í”„ ì´ë¯¸ì§€ ìƒì„±)
# ============================================
def visualize_with_torchviz():
    """torchvizë¥¼ ì‚¬ìš©í•œ computational graph ìƒì„±"""
    try:
        from torchviz import make_dot

        print("\n" + "=" * 80)
        print("ğŸ¨ Generating Computational Graph (torchviz)")
        print("=" * 80)

        # ëª¨ë¸ ìƒì„±
        model = ConcentrationSeq2Seq_v2(
            hidden_channels=32,
            num_lstm_layers=2,
            output_shape=(21, 45, 45)
        )

        # ì‘ì€ ì…ë ¥ìœ¼ë¡œ forward pass
        batch_size = 2
        past_conc = torch.randn(batch_size, 30, 21, 45, 45)
        static_maps = torch.randn(batch_size, 2, 21, 45, 45)

        # Forward
        output = model(past_conc, static_maps)

        # Computational graph ìƒì„±
        dot = make_dot(
            output,
            params=dict(model.named_parameters()),
            show_attrs=True,
            show_saved=True
        )

        # ì €ì¥
        output_path = "model_graph"
        dot.render(output_path, format='png', cleanup=True)

        print(f"âœ… Computational graph saved to: {output_path}.png")
        return True

    except ImportError:
        print("âš ï¸ torchviz not installed. Install with: pip install torchviz graphviz")
        print("   Also install Graphviz: https://graphviz.org/download/")
        return False
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False


# ============================================
# Method 3: Custom ASCII Visualization
# ============================================
def visualize_custom_ascii():
    """ì»¤ìŠ¤í…€ ASCII ì•„íŠ¸ë¡œ êµ¬ì¡° ì‹œê°í™”"""
    print("\n" + "=" * 80)
    print("ğŸ“ Custom Model Architecture Diagram")
    print("=" * 80)

    diagram = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ConcentrationSeq2Seq_v2 Architecture                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT LAYER                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  past_conc:    (B=8, T=30, D=21, H=45, W=45)  â”€â”€â”€â”€â”€â”                       â”‚
â”‚                                                     â”‚                       â”‚
â”‚  static_maps:  (B=8, C=2,  D=21, H=45, W=45)  â”€â”€â”  â”‚                       â”‚
â”‚                                                  â”‚  â”‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚  â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                â”‚                                     â”‚
                â–¼                                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  StaticEncoder        â”‚          â”‚  ConvLSTMEncoder       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                       â”‚          â”‚                        â”‚
    â”‚  Conv3d(2â†’16)         â”‚          â”‚  input_conv:           â”‚
    â”‚  BatchNorm + ReLU     â”‚          â”‚    Conv3d(1â†’32)        â”‚
    â”‚                       â”‚          â”‚    BatchNorm + ReLU    â”‚
    â”‚  Conv3d(16â†’32)        â”‚          â”‚                        â”‚
    â”‚  BatchNorm + ReLU     â”‚          â”‚  ConvLSTM Layer 1:     â”‚
    â”‚                       â”‚          â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚                       â”‚          â”‚    â”‚  for t=0..29â”‚    â”‚
    â”‚                       â”‚          â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
    â”‚                       â”‚          â”‚    â”‚  â”‚ h_t, c â”‚ â”‚    â”‚
    â”‚                       â”‚          â”‚    â”‚  â”‚  LSTM  â”‚ â”‚    â”‚
    â”‚                       â”‚          â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
    â”‚                       â”‚          â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                       â”‚          â”‚                        â”‚
    â”‚                       â”‚          â”‚  ConvLSTM Layer 2:     â”‚
    â”‚                       â”‚          â”‚    (same structure)    â”‚
    â”‚                       â”‚          â”‚                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                                   â”‚
                â”‚  (B, 32, 21, 45, 45)              â”‚  (B, 32, 21, 45, 45)
                â”‚                                   â”‚
                â”‚                                   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     UNetDecoder            â”‚
                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                â”‚                            â”‚
                â”‚  Fusion Layer:             â”‚
                â”‚    concat â†’ (B, 64, ...)   â”‚
                â”‚    Conv3d(64â†’32)           â”‚
                â”‚    BatchNorm + ReLU        â”‚
                â”‚                            â”‚
                â”‚  Decoder:                  â”‚
                â”‚    Conv3d(32â†’32)           â”‚
                â”‚    BatchNorm + ReLU        â”‚
                â”‚                            â”‚
                â”‚    Conv3d(32â†’16)           â”‚
                â”‚    BatchNorm + ReLU        â”‚
                â”‚                            â”‚
                â”‚    Conv3d(16â†’1)            â”‚
                â”‚                            â”‚
                â”‚  softplus (non-negative)   â”‚
                â”‚                            â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  OUTPUT                    â”‚
                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                â”‚  pred_conc:                â”‚
                â”‚  (B=8, 1, D=21, H=45, W=45)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Key Components:                                                               â•‘
â•‘                                                                                â•‘
â•‘  1. ConvLSTMEncoder: Processes 30 timesteps sequentially                      â•‘
â•‘     - Maintains spatial structure (21Ã—45Ã—45) throughout                       â•‘
â•‘     - LSTM hidden state captures temporal dependencies                        â•‘
â•‘                                                                                â•‘
â•‘  2. StaticEncoder: Encodes terrain and emission sources                       â•‘
â•‘     - Simple 2-layer Conv3d                                                   â•‘
â•‘                                                                                â•‘
â•‘  3. UNetDecoder: Fuses temporal + static features                             â•‘
â•‘     - Concatenates both feature maps                                          â•‘
â•‘     - Decodes to final prediction                                             â•‘
â•‘                                                                                â•‘
â•‘  Total Parameters: ~402,849                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

    print(diagram)

    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    model = ConcentrationSeq2Seq_v2(
        hidden_channels=32,
        num_lstm_layers=2,
        output_shape=(21, 45, 45)
    )

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print("\n" + "=" * 80)
    print("ğŸ“Š Parameter Statistics")
    print("=" * 80)
    print(f"Total parameters:      {total_params:,}")
    print(f"Trainable parameters:  {trainable_params:,}")
    print(f"Model size (approx):   {total_params * 4 / 1024 / 1024:.2f} MB (float32)")

    # ê° ì»´í¬ë„ŒíŠ¸ë³„ íŒŒë¼ë¯¸í„° ìˆ˜
    print("\n" + "-" * 80)
    print("Component-wise Parameters:")
    print("-" * 80)

    for name, module in model.named_children():
        num_params = sum(p.numel() for p in module.parameters())
        print(f"{name:30s}: {num_params:>10,} parameters")


# ============================================
# Method 4: Layer-by-layer breakdown
# ============================================
def visualize_layer_details():
    """ë ˆì´ì–´ë³„ ìƒì„¸ ì •ë³´ ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ğŸ” Layer-by-Layer Breakdown")
    print("=" * 80)

    model = ConcentrationSeq2Seq_v2(
        hidden_channels=32,
        num_lstm_layers=2,
        output_shape=(21, 45, 45)
    )

    print("\n1ï¸âƒ£  ConvLSTMEncoder (temporal_encoder)")
    print("-" * 80)
    for name, layer in model.temporal_encoder.named_modules():
        if isinstance(layer, (torch.nn.Conv3d, torch.nn.BatchNorm3d)):
            print(f"  {name:40s}: {layer}")

    print("\n2ï¸âƒ£  StaticEncoder (static_encoder)")
    print("-" * 80)
    for name, layer in model.static_encoder.named_modules():
        if isinstance(layer, (torch.nn.Conv3d, torch.nn.BatchNorm3d)):
            print(f"  {name:40s}: {layer}")

    print("\n3ï¸âƒ£  UNetDecoder (decoder)")
    print("-" * 80)
    for name, layer in model.decoder.named_modules():
        if isinstance(layer, (torch.nn.Conv3d, torch.nn.BatchNorm3d)):
            print(f"  {name:40s}: {layer}")


# ============================================
# Main execution
# ============================================
def main():
    print("\n" + "â•”" + "â•" * 78 + "â•—")
    print("â•‘" + " " * 20 + "MODEL VISUALIZATION SUITE" + " " * 33 + "â•‘")
    print("â•š" + "â•" * 78 + "â•\n")

    # Method 1: torchinfo (ê¶Œì¥)
    success_torchinfo = visualize_with_torchinfo()

    # Method 2: Custom ASCII
    visualize_custom_ascii()

    # Method 3: Layer details
    visualize_layer_details()

    # Method 4: torchviz (ì„ íƒì )
    print("\n" + "=" * 80)
    print("Would you like to generate computational graph image? (requires graphviz)")
    print("This will create a PNG file showing the forward pass.")
    print("=" * 80)

    # ìë™ìœ¼ë¡œ ì‹œë„
    visualize_with_torchviz()

    print("\n" + "=" * 80)
    print("âœ… Visualization Complete!")
    print("=" * 80)

    if not success_torchinfo:
        print("\nğŸ’¡ Tip: Install torchinfo for detailed layer information:")
        print("   pip install torchinfo")

    print("\nğŸ’¡ Tip: For computational graph (PNG image):")
    print("   1. Install: pip install torchviz")
    print("   2. Install Graphviz: https://graphviz.org/download/")
    print("   3. Add Graphviz to PATH")


if __name__ == "__main__":
    main()
